{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio file example and compute mel spectrogram\n",
    "Will plot it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_spectrogram.shape=(5, 1201)\n"
     ]
    }
   ],
   "source": [
    "# Load audio file example\n",
    "audio_file = '84_121123_000069_000000.wav'\n",
    "\n",
    "audio, sr = librosa.load(audio_file, sr=22050)  # Adjust sr as needed\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=256, n_mels=5)\n",
    "mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "print(f'{mel_spectrogram.shape=}')\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.imshow(mel_spectrogram, aspect='auto', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.savefig('mel_spectrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Make encoder and decoder using Tacotron2 model\n",
    "Taken as \"Tacotron2().encoder\" and \"Tacotron2().decoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Init speaker_embedding layer.\n"
     ]
    }
   ],
   "source": [
    "current_path = os.path.dirname(os.getcwd())\n",
    "output_path = os.path.join(current_path, \"runs\")\n",
    "\n",
    "# define model config\n",
    "config = Tacotron2Config(\n",
    "    batch_size=4,\n",
    "    eval_batch_size=4,\n",
    "    num_loader_workers=0,\n",
    "    num_eval_loader_workers=0,\n",
    "    precompute_num_workers=0,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=1,\n",
    "    print_step=1,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    # datasets=[dataset_config],\n",
    "    use_speaker_embedding=True,\n",
    "    min_text_len=0,\n",
    "    max_text_len=500,\n",
    "    min_audio_len=0,\n",
    "    max_audio_len=500000,\n",
    ")\n",
    "\n",
    "ap = AudioProcessor.init_from_config(config, verbose=False)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "tacotron2_model = Tacotron2(config, ap, tokenizer, speaker_manager=SpeakerManager())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_spectrogram.shape=(5, 1201)\n",
      "reshaped_mel_spectrogram.shape=torch.Size([1, 512, 1201]), input_lengths=tensor([1201])\n",
      "memories.shape=torch.Size([1, 1201, 512]), memories.dtype=torch.float32\n",
      "image.shape=(512, 1201)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "mel_spectrogram_length = mel_spectrogram.shape[1]\n",
    "encoder = tacotron2_model.encoder\n",
    "\n",
    "# Assuming mel_spectrogram is your preprocessed mel spectrogram\n",
    "reshaped_mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "print(f'{mel_spectrogram.shape=}')\n",
    "\n",
    "# Reshape mel spectrogram to (512, n) using interpolation\n",
    "target_shape = (512, mel_spectrogram_length)\n",
    "reshaped_mel_spectrogram = reshaped_mel_spectrogram.unsqueeze(0).permute(0, 2, 1)\n",
    "reshaped_mel_spectrogram = TF.resize(reshaped_mel_spectrogram, size=target_shape, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "\n",
    "input_lengths = torch.tensor([reshaped_mel_spectrogram.shape[-1]])\n",
    "encoder.in_out_channels = input_lengths.detach().cpu().numpy()[0]\n",
    "print(f'{reshaped_mel_spectrogram.shape=}, {input_lengths=}')\n",
    "\n",
    "# Pass mel spectrogram and input lengths through the encoder\n",
    "memories = encoder(reshaped_mel_spectrogram, input_lengths)\n",
    "print(f'{memories.shape=}, {memories.dtype=}')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "image = memories.permute(2, 1, 0).detach().numpy()\n",
    "image = np.reshape(image, (image.shape[1], image.shape[0])).T\n",
    "print(f'{image.shape=}')\n",
    "plt.imshow(image, aspect='auto', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.savefig('embedding2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decoder analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of dummy input tensors:\n",
      "Embedding: torch.Size([1, 512, 1024])\n",
      "Memories: torch.Size([1, 64, 160])\n",
      "\n",
      "Element 0\n",
      "Content: [-0.01196379  0.12897922 -0.02115425  0.14590442 -0.03939753] ...\n",
      "Shape: torch.Size([1, 80, 128])\n",
      "\n",
      "Element 1\n",
      "Content: [0.00066258 0.00251465 0.00038548 0.00262291 0.00081881] ...\n",
      "Shape: torch.Size([1, 64, 512])\n",
      "\n",
      "Element 2\n",
      "Content: [-0.00600709 -0.0101943  -0.0421669  -0.08888654 -0.08210397] ...\n",
      "Shape: torch.Size([1, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "e = 512\n",
    "m = 64\n",
    "\n",
    "# Create dummy input tensors\n",
    "dummy_embedding = torch.randn(1, e, 1024)\n",
    "dummy_memories = torch.randn(1, m, 160)\n",
    "\n",
    "# Print shapes of dummy input tensors\n",
    "print(\"Shapes of dummy input tensors:\")\n",
    "print(\"Embedding:\", dummy_embedding.shape)\n",
    "print(\"Memories:\", dummy_memories.shape)\n",
    "print()\n",
    "\n",
    "# Pass the dummy input tensors to the decoder\n",
    "dummy_decoder_outputs = tacotron2_model.decoder(\n",
    "    dummy_embedding,\n",
    "    memories=dummy_memories,\n",
    "    mask=None,\n",
    ")\n",
    "\n",
    "for i, content in enumerate(dummy_decoder_outputs):\n",
    "    print(f'Element {i}')\n",
    "    example = content.detach().cpu().numpy().flatten()\n",
    "    print('Content:', example[:5], '...' if len(example) > 1 else '')\n",
    "    print('Shape:', content.shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of input tensors:\n",
      "Embedding: torch.Size([1, 1, 1024])\n",
      "Memories: torch.Size([1, 1201, 160])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Pass the input tensors to the decoder\u001b[39;00m\n\u001b[0;32m     14\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m tacotron2_model\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m     15\u001b[0m     dummy_embedding,\n\u001b[0;32m     16\u001b[0m     memories\u001b[38;5;241m=\u001b[39mmemories,\n\u001b[0;32m     17\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtacotron2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(decoder_outputs):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tonko\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\TTS\\tts\\layers\\tacotron\\tacotron2.py:263\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03mshapes:\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m   - memory: B x r * self.frame_channels\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# self.context: B x D_en\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# query_input: B x D_en + (r * self.frame_channels)\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m query_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# self.query and self.attention_rnn_cell_state : B x D_attn_rnn\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_rnn_cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_rnn(\n\u001b[0;32m    266\u001b[0m     query_input, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_rnn_cell_state)\n\u001b[0;32m    267\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "dummy_embedding = torch.randn(1, 1, 1024)\n",
    "embedding = dummy_embedding\n",
    "memories = memories[:, :, :160]\n",
    "\n",
    "# Print shapes of input tensors\n",
    "print(\"Shapes of input tensors:\")\n",
    "print(\"Embedding:\", embedding.shape)\n",
    "print(\"Memories:\", memories.shape)\n",
    "print()\n",
    "\n",
    "# Pass the input tensors to the decoder\n",
    "decoder_outputs = tacotron2_model.decoder(\n",
    "    dummy_embedding,\n",
    "    memories=memories,\n",
    "    mask=None,\n",
    ")\n",
    "\n",
    "print(tacotron2_model.decoder.decode(embedding))\n",
    "\n",
    "for i, content in enumerate(decoder_outputs):\n",
    "    print(f'Element {i}')\n",
    "    example = content.detach().cpu().numpy().flatten()\n",
    "    print('Content:', example[:5], '...' if len(example) > 1 else '')\n",
    "    print('Shape:', content.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stand alone Decoder analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of dummy input tensors:\n",
      "Embedding: torch.Size([1, 512, 1024])\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Decoder.__init__() missing 15 required positional arguments: 'in_channels', 'frame_channels', 'r', 'attn_type', 'attn_win', 'attn_norm', 'prenet_type', 'prenet_dropout', 'forward_attn', 'trans_agent', 'forward_attn_mask', 'location_attn', 'attn_K', 'separate_stopnet', and 'max_decoder_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Pass the dummy input tensors to the decoder\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(dummy_embedding)\n",
      "\u001b[1;31mTypeError\u001b[0m: Decoder.__init__() missing 15 required positional arguments: 'in_channels', 'frame_channels', 'r', 'attn_type', 'attn_win', 'attn_norm', 'prenet_type', 'prenet_dropout', 'forward_attn', 'trans_agent', 'forward_attn_mask', 'location_attn', 'attn_K', 'separate_stopnet', and 'max_decoder_steps'"
     ]
    }
   ],
   "source": [
    "from TTS.tts.layers.tacotron.tacotron2 import Decoder\n",
    "\n",
    "e = 512\n",
    "m = 64\n",
    "\n",
    "# Create dummy input tensors\n",
    "dummy_embedding = torch.randn(1, e, 1024)\n",
    "\n",
    "# Print shapes of dummy input tensors\n",
    "print(\"Shapes of dummy input tensors:\")\n",
    "print(\"Embedding:\", dummy_embedding.shape)\n",
    "print()\n",
    "\n",
    "# Pass the dummy input tensors to the decoder\n",
    "decoder = Decoder()\n",
    "decoder_outputs = decoder.decode(dummy_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio processor and speaker encoder\n",
    "ap = AudioProcessor(**config.audio)\n",
    "\n",
    "manager = SpeakerManager(encoder_model_path=encoder_model_path, encoder_config_path=encoder_config_path)\n",
    "\n",
    "# load a sample audio and compute embedding\n",
    "waveform = ap.load_wav(sample_wav_path)\n",
    "\n",
    "mel = ap.melspectrogram(waveform)\n",
    "\n",
    "d_vector = manager.compute_embeddings(mel.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming mel_spectrogram is your preprocessed mel spectrogram\n",
    "reshaped_mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "\n",
    "# Reshape mel spectrogram to (512, 512) using interpolation\n",
    "target_shape = (512, 512)\n",
    "reshaped_mel_spectrogram = reshaped_mel_spectrogram.unsqueeze(0).permute(0, 2, 1)\n",
    "reshaped_mel_spectrogram = TF.resize(reshaped_mel_spectrogram, size=target_shape, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "\n",
    "input_lengths = torch.tensor([reshaped_mel_spectrogram.shape[-1]])\n",
    "encoder_output_tensor  = tacotron2_model.encoder(reshaped_mel_spectrogram, input_lengths)\n",
    "print(encoder_output_tensor.shape)\n",
    "\n",
    "# # Extract the embedding\n",
    "# embedding = encoder_output_tensor[\"encoder_out\"]\n",
    "dummy_embedding = encoder_output_tensor\n",
    "print(dummy_embedding.shape)\n",
    "\n",
    "# Pass the embedding through the decoder\n",
    "decoder_outputs, _ = tacotron2_model.decoder(\n",
    "    dummy_embedding, memories=encoder_output_tensor[:, :160, :160], mask=None)\n",
    "\n",
    "# Get the predicted mel spectrogram from the decoder outputs\n",
    "predicted_mel_spectrogram = decoder_outputs[\"mel_outputs\"]\n",
    "\n",
    "# You can also get the stop token predictions if needed\n",
    "stop_token_predictions = decoder_outputs[\"stop_token_predictions\"]\n",
    "\n",
    "print(predicted_mel_spectrogram.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
