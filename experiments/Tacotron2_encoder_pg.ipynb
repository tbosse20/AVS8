{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio file example and compute mel spectrogram\n",
    "Will plot it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file example\n",
    "audio_file = '84_121123_000069_000000.wav'\n",
    "\n",
    "audio, sr = librosa.load(audio_file, sr=22050)  # Adjust sr as needed\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024, hop_length=256, n_mels=5)\n",
    "mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "print(f'{mel_spectrogram.shape=}')\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram, aspect='auto', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.savefig('mel_spectrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Make encoder and decoder using Tacotron2 model\n",
    "Taken as \"Tacotron2().encoder\" and \"Tacotron2().decoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "output_path = os.path.join(current_path, \"runs\")\n",
    "\n",
    "# define model config\n",
    "config = Tacotron2Config(\n",
    "    batch_size=4,\n",
    "    eval_batch_size=4,\n",
    "    num_loader_workers=0,\n",
    "    num_eval_loader_workers=0,\n",
    "    precompute_num_workers=0,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=1,\n",
    "    print_step=1,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    # datasets=[dataset_config],\n",
    "    use_speaker_embedding=True,\n",
    "    min_text_len=0,\n",
    "    max_text_len=500,\n",
    "    min_audio_len=0,\n",
    "    max_audio_len=500000,\n",
    ")\n",
    "\n",
    "ap = AudioProcessor.init_from_config(config, verbose=False)\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "tacotron2_model = Tacotron2(config, ap, tokenizer, speaker_manager=SpeakerManager())\n",
    "\n",
    "# Assuming mel_spectrogram is your preprocessed mel spectrogram\n",
    "reshaped_mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "\n",
    "# Reshape mel spectrogram to (512, 512) using interpolation\n",
    "target_shape = (512, 512)\n",
    "reshaped_mel_spectrogram = reshaped_mel_spectrogram.unsqueeze(0).permute(0, 2, 1)\n",
    "reshaped_mel_spectrogram = TF.resize(reshaped_mel_spectrogram, size=target_shape, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "\n",
    "input_lengths = torch.tensor([reshaped_mel_spectrogram.shape[-1]])\n",
    "encoder_output_tensor  = tacotron2_model.encoder(reshaped_mel_spectrogram, input_lengths)\n",
    "print(encoder_output_tensor.shape)\n",
    "\n",
    "# # Extract the embedding\n",
    "# embedding = encoder_output_tensor[\"encoder_out\"]\n",
    "embedding = encoder_output_tensor\n",
    "print(embedding.shape)\n",
    "\n",
    "# Pass the embedding through the decoder\n",
    "decoder_outputs, _ = tacotron2_model.decoder(\n",
    "    embedding, memories=encoder_output_tensor[:, :160, :160], mask=None)\n",
    "\n",
    "# Get the predicted mel spectrogram from the decoder outputs\n",
    "predicted_mel_spectrogram = decoder_outputs[\"mel_outputs\"]\n",
    "\n",
    "# You can also get the stop token predictions if needed\n",
    "stop_token_predictions = decoder_outputs[\"stop_token_predictions\"]\n",
    "\n",
    "print(predicted_mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decoder analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create dummy input tensors\n",
    "embedding = torch.randn(1, 1024, 1024)\n",
    "memories = torch.randn(1, 1024, 160)\n",
    "\n",
    "# Print shapes of dummy input tensors\n",
    "print(\"Shapes of dummy input tensors:\")\n",
    "print(\"Embedding:\", embedding.shape)\n",
    "print(\"Memories:\", memories.shape)\n",
    "print()\n",
    "\n",
    "# Pass the dummy input tensors to the decoder\n",
    "decoder_outputs = tacotron2_model.decoder(\n",
    "    embedding,\n",
    "    memories=memories,\n",
    "    mask=None,\n",
    ")\n",
    "\n",
    "for i, content in enumerate(decoder_outputs):\n",
    "    print(f'Element {i}')\n",
    "    example = content.detach().cpu().numpy().flatten()\n",
    "    print('Content:', example[:5], '...' if len(example) > 1 else '')\n",
    "    print('Shape:', content.shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from TTS.tts.layers.tacotron.tacotron2 import Encoder\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "encoder = Encoder()\n",
    "\n",
    "# Assuming mel_spectrogram is your preprocessed mel spectrogram\n",
    "reshaped_mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "\n",
    "# Reshape mel spectrogram to (512, 512) using interpolation\n",
    "target_shape = (512, 512)\n",
    "reshaped_mel_spectrogram = reshaped_mel_spectrogram.unsqueeze(0).permute(0, 2, 1)\n",
    "reshaped_mel_spectrogram = TF.resize(reshaped_mel_spectrogram, size=target_shape, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "\n",
    "input_lengths = torch.tensor([reshaped_mel_spectrogram.shape[-1]])\n",
    "encoder.in_out_channels = input_lengths.detach().cpu().numpy()[0]\n",
    "\n",
    "# Pass mel spectrogram and input lengths through the encoder\n",
    "embedding = encoder(reshaped_mel_spectrogram, input_lengths)\n",
    "print(f'{embedding.shape=}')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(embedding.permute(1, 2, 0).detach().numpy(), aspect='auto', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.savefig('embedding.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from TTS.tts.layers.tacotron.tacotron2 import Decoder\n",
    "\n",
    "decoder = Decoder()\n",
    "synth = decoder(embedding, mel_spectrogram.shape[-1])\n",
    "print(synth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio processor and speaker encoder\n",
    "ap = AudioProcessor(**config.audio)\n",
    "\n",
    "manager = SpeakerManager(encoder_model_path=encoder_model_path, encoder_config_path=encoder_config_path)\n",
    "\n",
    "# load a sample audio and compute embedding\n",
    "waveform = ap.load_wav(sample_wav_path)\n",
    "\n",
    "mel = ap.melspectrogram(waveform)\n",
    "\n",
    "d_vector = manager.compute_embeddings(mel.T)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
